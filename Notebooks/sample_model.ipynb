{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c7208c",
   "metadata": {},
   "source": [
    "# Testing Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eceacf",
   "metadata": {},
   "source": [
    "Versions\n",
    "\n",
    "- 01\n",
    "    - Testing for HuggingFace Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38313aff",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f3e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.nn import Module # For type hinting\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbefac4",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de220668",
   "metadata": {},
   "source": [
    "### Creating Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdbbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OilDataset(Dataset):\n",
    "    \"\"\"Dataset class For the OIL_DATASET\"\"\"\n",
    "    def __init__(self, csv_file=\"../Data/DataSplits/test.csv\"):\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)   # Assign a pandas data frame\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found: {csv_file}\")\n",
    "\n",
    "        # Define feature and label columns\n",
    "        self.label_column = \"Close\"\n",
    "        # Remove the label column from the feature columns\n",
    "        self.feature_columns = self.data.columns.drop([self.label_column])\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data.loc[index, self.feature_columns].values    # Extract the features for the given index\n",
    "        \n",
    "        label = self.data.loc[index, self.label_column] # Extract the label for the given index\n",
    "        return (\n",
    "            torch.tensor(features, dtype=torch.float),\n",
    "            torch.tensor(label, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9b5c1",
   "metadata": {},
   "source": [
    "### Data Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(root_data_dir: str= \"../Data\", data_file_path: str=\"OIL_DATASET.csv\", data_splits_dir: str=\"DataSplits\", scaler_dir = \"Scalers\", batch_size: int=64, num_workers=0, pin_memory: bool=False, drop_last: bool=True) -> tuple[Dataset, Dataset, Dataset, DataLoader, DataLoader, DataLoader, MinMaxScaler, MinMaxScaler]:\n",
    "    \"\"\"This function prepares the train, test, and validation datasets.\n",
    "    Args:\n",
    "        root_data_dir (str): The root of the Data Directory\n",
    "        data_file_path (str): The name of the original dataset (with .csv file extension).\n",
    "        data_splits_dir (str): Path to the train, test, and validation datasets.\n",
    "        scaler_dir (str): Path to the feature and label scalers.\n",
    "        batch_size (int): The dataloader's batch_size.\n",
    "        num_workers (int): The dataloader's number of workers.\n",
    "        pin_memory (bool): The dataloader's pin memory option.\n",
    "        drop_last (bool): The dataloader's drop_last option.\n",
    "\n",
    "    Returns: \n",
    "        train_dataset (Dataset): Dataset Class for the training dataset.\n",
    "        test_dataset (Dataset): Dataset Class for the test dataset.\n",
    "        validation_dataset (Dataset): Dataset Class for the validation dataset.\n",
    "        train_dataloader (DataLoader): The train dataloader.\n",
    "        test_dataloader (DataLoader): The test dataloader.\n",
    "        validation_dataloader (DataLoader): The validation dataloader.\n",
    "        feature_scaler (MinMaxScaler): The scaler used to scale the features of the model input.\n",
    "        label_scaler (MinMaxScaler): The scaler used to scale the labels of the model input.\n",
    "        \"\"\"\n",
    "    if not root_data_dir or not data_file_path or not data_splits_dir:  # Check for empty strings at the beginning\n",
    "        raise ValueError(\"File and directory paths cannot be empty strings.\")\n",
    "    DATA_ROOT = Path(root_data_dir)\n",
    "    # OIL_PATH_ORIGINAL = DATA_ROOT / \"OIL_Dataset_1984-2025.csv\"     # Set the data source path\n",
    "\n",
    "    DATA_CLEAN_PATH = DATA_ROOT / data_file_path # Set the path to the complete dataset\n",
    "\n",
    "    if DATA_CLEAN_PATH.exists():\n",
    "        print(f\"CSV file detected, reading from '{DATA_ROOT}'\")\n",
    "        df = pd.read_csv(DATA_CLEAN_PATH)\n",
    "    else:\n",
    "        print(f\"Downloading CSV file from HuggingFace\")\n",
    "        os.makedirs(DATA_ROOT, exist_ok=True)       # Create the Data Root Directory\n",
    "        df = pd.read_csv(\"hf://datasets/MaxPrestige/CRUDE_OIL_PRICES/Data/OIL_DATASET.csv\")  # Download and read the data into a pandas dataframe\n",
    "        df.to_csv(DATA_CLEAN_PATH, index=False)     # Save the file, omitting saving the index\n",
    "\n",
    "    DATA_SPLITS_DIR = DATA_ROOT / data_splits_dir\n",
    "    SCALER_DIR = DATA_ROOT / scaler_dir\n",
    "\n",
    "    TRAIN_DATA_PATH = DATA_SPLITS_DIR / \"train.csv\"\n",
    "    TEST_DATA_PATH = DATA_SPLITS_DIR / \"test.csv\"\n",
    "    VALIDATION_DATA_PATH = DATA_SPLITS_DIR / \"val.csv\"\n",
    "\n",
    "    FEATURE_SCALER_PATH = SCALER_DIR / \"feature-scaler.joblib\"\n",
    "    LABEL_SCALER_PATH = SCALER_DIR / \"label-scaler.joblib\"\n",
    "    \n",
    "    label_col = \"Close\"\n",
    "    extra_dropped_cols = 'Date'\n",
    "\n",
    "    if os.path.exists(TRAIN_DATA_PATH) and os.path.exists(TEST_DATA_PATH) and os.path.exists(VALIDATION_DATA_PATH) :\n",
    "        print(f\"Train, Test, and Validation csv datasets detected in '{DATA_SPLITS_DIR}.' Skipping generation and loading scaler(s)\")\n",
    "        try:\n",
    "            feature_scaler = joblib.load(FEATURE_SCALER_PATH)\n",
    "            label_scaler = joblib.load(LABEL_SCALER_PATH)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when loading scalers: {e}\")\n",
    "    else:\n",
    "        print(f\"Datasets not found in '{DATA_SPLITS_DIR}' or incomplete. Generating datasets...\")\n",
    "        # os.makedirs(MODEL_ROOT, exist_ok=True)\n",
    "        os.makedirs(DATA_SPLITS_DIR, exist_ok=True)     # Create the Data Splits Parent Directory\n",
    "        os.makedirs(SCALER_DIR, exist_ok=True)     # Create the Scaler Parent Directory\n",
    "\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        label_scaler = MinMaxScaler()\n",
    "        # Split the Dataframe into separate features and labels DataFrames\n",
    "        df_features = df.drop(columns=[label_col, extra_dropped_cols], inplace=False)\n",
    "        df_labels = df[[label_col]]     # Instead of returning a pandas Series using \"[]\", return a dataframe using the \"[[]]\" to get a shape with (-1,1)\n",
    "\n",
    "        # Split into smaller DataFrames for the Train, Test, and Validation splits\n",
    "        X_train, X_inter, Y_train, Y_inter = train_test_split(df_features, df_labels, test_size=0.1, random_state=42)\n",
    "        X_validation, X_test, Y_validation, Y_test = train_test_split(X_inter, Y_inter, test_size=0.5, random_state=42)\n",
    "\n",
    "        feature_scaler.fit(X_train)\n",
    "        label_scaler.fit(Y_train)\n",
    "\n",
    "        # Save the fitted scaler object\n",
    "        try:\n",
    "            joblib.dump(feature_scaler, FEATURE_SCALER_PATH)\n",
    "            print(f\"Feature scaler stored in: ({FEATURE_SCALER_PATH})\")\n",
    "            joblib.dump(label_scaler, LABEL_SCALER_PATH)\n",
    "            print(f\"Label scaler stored in: ({LABEL_SCALER_PATH})\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when saving  Scalers: {e}\")\n",
    "\n",
    "        # Scale the rest of the data; returns numpy arrays\n",
    "        X_train_scaled = feature_scaler.transform(X_train)\n",
    "        Y_train_scaled = label_scaler.transform(Y_train)\n",
    "        X_validation_scaled = feature_scaler.transform(X_validation)\n",
    "        Y_validation_scaled = label_scaler.transform(Y_validation)\n",
    "        X_test_scaled = feature_scaler.transform(X_test)\n",
    "        Y_test_scaled = label_scaler.transform(Y_test)\n",
    "\n",
    "        print(f\"Train Features Scaled Shape: {X_train_scaled.shape}\")\n",
    "        print(f\"Train Labels Scaled Shape: {Y_test_scaled.shape}\")\n",
    "        print(f\"validation Features Scaled Shape: {X_validation_scaled.shape}\")\n",
    "        print(f\"validation Labels: {Y_validation_scaled.shape}\")\n",
    "        print(f\"test Features Scaled Shape: {X_test_scaled.shape}\")\n",
    "        print(f\"test Labels Scaled Shape: {Y_test_scaled.shape}\")\n",
    "        # Define the column names of the features and label\n",
    "        features_names = df_features.columns\n",
    "        label_name = df_labels.columns\n",
    "        # Create dataframes using the scaled data\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, columns=features_names)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, columns=features_names)\n",
    "        X_validation_df = pd.DataFrame(X_validation_scaled, columns=features_names)\n",
    "        Y_train_df = pd.DataFrame(Y_train_scaled, columns=label_name)\n",
    "        Y_test_df = pd.DataFrame(Y_test_scaled, columns=label_name)\n",
    "        Y_validation_df = pd.DataFrame(Y_validation_scaled, columns=label_name)\n",
    "\n",
    "        # Concatenate the features and labels back into a single DataFrame for each set\n",
    "        train_data_frame = pd.concat([X_train_df, Y_train_df.reset_index(drop=True)], axis=1)\n",
    "        test_data_frame = pd.concat([X_test_df, Y_test_df.reset_index(drop=True)], axis=1)\n",
    "        validation_data_frame = pd.concat([X_validation_df, Y_validation_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # Saving the split data to csv files\n",
    "        train_data_frame.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "        test_data_frame.to_csv(TEST_DATA_PATH, index=False)\n",
    "        validation_data_frame.to_csv(VALIDATION_DATA_PATH, index=False)\n",
    "    # Creating Datasets from the stored datasets\n",
    "    print(f\"Initializing Datasets\")\n",
    "    train_dataset = OilDataset(TRAIN_DATA_PATH)\n",
    "    test_dataset = OilDataset(TEST_DATA_PATH)\n",
    "    val_dataset = OilDataset(VALIDATION_DATA_PATH)\n",
    "    \n",
    "    print(f\"Creating DataLoaders with batch_size ({batch_size}), num_workers ({num_workers}), pin_memory ({pin_memory}). Training dataset drop_last: ({drop_last})\")\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last, shuffle=True)\n",
    "    validation_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last, shuffle=False)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last, shuffle=False)\n",
    "\n",
    "    print(f\"Training DataLoader has ({len(train_dataloader)}) batches, Test DataLoader has ({len(test_dataloader)}) batches, Validation DataLoader has ({len(validation_dataloader)}) batches\")\n",
    "\n",
    "    return (train_dataset, test_dataset, val_dataset, train_dataloader, test_dataloader, validation_dataloader, feature_scaler, label_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3838d663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file detected, reading from '..\\Data'\n",
      "Train, Test, and Validation csv datasets detected in '..\\Data\\DataSplits.' Skipping generation and loading scaler(s)\n",
      "Initializing Datasets\n",
      "Creating DataLoaders with batch_size (64), num_workers (0), pin_memory (False). Training dataset drop_last: (True)\n",
      "Training DataLoader has (89) batches, Test DataLoader has (4) batches, Validation DataLoader has (4) batches\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data_pipeline(root_data_dir=\"../Data\", data_file_path=\"OIL_DATASET.csv\", data_splits_dir=\"DataSplits\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"An unexpected error occurred when running the data pipeline function:{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e203af",
   "metadata": {},
   "source": [
    "## Agent Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e22b8",
   "metadata": {},
   "source": [
    "### Module Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955cfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleLayer(torch.nn.Module):\n",
    "    \"\"\"Class for the individual layer blocks.\"\"\"\n",
    "    def __init__(self, intermediate_dim=32, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mod_linear = torch.nn.Linear(intermediate_dim, intermediate_dim)\n",
    "        self.mod_norm = torch.nn.LayerNorm(normalized_shape=intermediate_dim)\n",
    "        self.mod_relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the layer block.\"\"\"\n",
    "        residual = x\n",
    "        x = self.mod_linear(x)\n",
    "        x = self.mod_norm(x)\n",
    "        x = self.mod_relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9c2e5",
   "metadata": {},
   "source": [
    "### Oil Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    \"\"\"Class for Agent Structure using multiple Layer Blocks.\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=cfg[\"in_dim\"], out_features=cfg[\"intermediate_dim\"])\n",
    "        self.layers = torch.nn.Sequential(*[ModuleLayer(intermediate_dim=cfg[\"intermediate_dim\"], dropout_rate=cfg[\"dropout_rate\"]) for _ in range(cfg[\"num_blocks\"])])\n",
    "        self.out = torch.nn.Linear(in_features=cfg[\"intermediate_dim\"], out_features=cfg[\"out_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def get_prediction(self, features):\n",
    "        \"\"\"Get the deterministic prediction on a single observation or a batch of observations.\n",
    "        \n",
    "        Parameters:\n",
    "            features (torch.tensor): the agent's input features. Expected shape is either `(num_features,)` for a single observation\n",
    "            or `(batch_size, num_features)` for a batch of observations.\n",
    "        \n",
    "        Returns:\n",
    "            action (int or torch.tensor): \n",
    "                - If `features` is a single features (i.e., `features.dim() == 1`), returns a scalar `int` representing the chosen action. \n",
    "\n",
    "                - If `features` is a batch of features (i.e., `features.dim() > 1`),\n",
    "                returns a `torch.Tensor` of `int`s, where each element is the\n",
    "                chosen action for the corresponding observation in the batch\"\"\"\n",
    "        # Ensure single samples have a batch dimension\n",
    "        if features.dim() == 1:\n",
    "            features = features.unsqueeze(0) # Add a batch dimension if it's a single batch of features\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(features, torch.Tensor):  # Check if features is not already a tensor\n",
    "                features = torch.tensor(features, dtype=torch.float32)\n",
    "            prediction = self.forward(features)  # Run a forward pass through the model\n",
    "        if features.size(0) == 1:    # This method checks if there is only 1 element in a 1-D tensor\n",
    "            return prediction.item() # Returns a Python scalar for a single observation\n",
    "        else:\n",
    "            return prediction # Returns a tensor of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b1106",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf679b",
   "metadata": {},
   "source": [
    "### Log Iteration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05d5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_iteration(batch_idx: int, total_batches: int, loss_value: float):\n",
    "    \"\"\"Logs the loss of the current batch.\"\"\"\n",
    "    print(f\"Epoch batch [{batch_idx}/{total_batches}] | Loss: {loss_value:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162971a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_epoch_iteration(epoch: int, avg_epoch_loss: float):\n",
    "    \"\"\"Log Current Metrics accumulated in the current epoch iteration.\n",
    "    Args:\n",
    "        epoch (int): the current iteration\n",
    "        avg_epoch_loss (float): The average loss of the current epoch\n",
    "    Returns:\n",
    "        N/A\n",
    "        \"\"\"\n",
    "    if avg_epoch_loss:\n",
    "        print(f\"=====================  [EPOCH ({epoch}) LOGGING]  =====================\")\n",
    "        print(\"| AVERAGES of THIS EPOCH:\")\n",
    "        print(f\"| ACCUMULATED LOSS: {avg_epoch_loss:.7f}\")\n",
    "        print(f\"===========================================================\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No Data collected for this epoch to log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44d5f8",
   "metadata": {},
   "source": [
    "### Evaluate Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58184545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: Module, dataloader: DataLoader, current_epoch: int = None, max_epochs: int=None, device: str = 'cpu') -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset and returns the average loss.\n",
    "    Args:\n",
    "        model (Module): The Model.\n",
    "        dataloader (DataLoader): The dataloader to calculate average loss with.\n",
    "        current_epoch (int): The current epoch [optional].\n",
    "        max_epochs (int): The maximum number of epochs [optional].\n",
    "        device (str): The device that the calculations will take place on.\n",
    "    Returns:\n",
    "        avg_loss (float): The calculated average loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    # loss_fn = torch.nn.MELoss(reduction='sum') # Use reduction='sum' instead of 'mean' for total loss\n",
    "    loss_fn = torch.nn.L1Loss(reduction='sum')\n",
    "    if len(dataloader.dataset) == 0:\n",
    "        print(\"Warning: Evaluation dataset is empty. Skipping evaluation.\")\n",
    "        return float('nan')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in dataloader:\n",
    "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.unsqueeze(dim=-1).to(device)\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader.dataset)     # Calculate the average loss on the dataset\n",
    "\n",
    "    if current_epoch and max_epochs:   # If the function was called in the training loop\n",
    "        print(f\"===================  [Epoch ({current_epoch}/{max_epochs})]  ===================\")\n",
    "        print(f\"Entire Validation Dataset Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"====================================================\")\n",
    "\n",
    "    else:   # If the function was called outside of the training loop\n",
    "        print(f\"===============================================\")\n",
    "        print(f\"Entire Dataset Average Loss: {avg_loss:.4f} \")\n",
    "        print(f\"=====================================================\")\n",
    "            \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44fbd9d",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_config: dict, train_dataloader: DataLoader, validation_dataloader: DataLoader, model: Agent = None, epochs=32, learning_rate=0.0003, max_grad_norm=0.5, log_iterations=10, eval_iterations=10, device=\"cpu\") -> Agent:\n",
    "    \"\"\"The Model Training function.\n",
    "\n",
    "    Args:\n",
    "        model_config (dict): The base configurations for building the policies.\n",
    "        train_dataloader (DataLoader): The dataloader for the training loop.\n",
    "        validation_dataloader (DataLoader): The dataloader for the validation loop.\n",
    "        model (Agent): The model to be trained.\n",
    "        epochs (int): The number of times the outer loop is performed.\n",
    "        learning_rate (float): The hyperparameter that affects how much the model's parameters learn on each update iteration.\n",
    "        max_grad_norm (float): Used to promote numerical stability and prevent exploding gradients.\n",
    "        log_iterations (int): Used to log information about the state of the Agent.\n",
    "        eval_iterations (int): Used to run an evaluation of the Agent.\n",
    "        device (str): The device that the model will be trained on.\n",
    "\n",
    "    Returns: \n",
    "        agent (Module): The Trained Model in evaluation mode.\n",
    "    \"\"\"\n",
    "    print(f\"Training Model on {device} with {epochs} main epochs, {learning_rate} learning rate, max_grad_norm={max_grad_norm}.\")\n",
    "    print(f\"Logging every {log_iterations} epoch iterations, evaluating every {eval_iterations} epoch iterations.\")\n",
    "\n",
    "    agent = (model if model is not None else Agent(model_config)).to(device) # Create agent if nothing was passed, otherwise, create the agent. Send agent to device.\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params=agent.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.L1Loss(reduction='mean')      # Define the Loss function\n",
    "\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    train_dataloader_length = len(train_dataloader)\n",
    "    agent.train()   # Set agent to training mode\n",
    "    for epoch in tqdm(range(epochs), desc=f\">>>>>>>>>>>>>>>>>>>>>\\nMain Epoch (Outer Loop)\", leave=True):\n",
    "\n",
    "        epoch_loss_total = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\", leave=False)):           # Get a mini-batch of training examples from the dataloader\n",
    "            # optimizer.zero_grad(set_to_none=True)       # Clear the gradients built up; Setting to None to improve performance\n",
    "            optimizer.zero_grad()       # Clear the gradients built up; Setting to None to improve performance\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.unsqueeze(dim=-1).to(device)   # Move the inputs and labels to the device\n",
    "\n",
    "            agent_outputs = agent(inputs)       # Pass the inputs to the model and get the outputs.\n",
    "\n",
    "            loss = loss_fn(agent_outputs, labels)      # Calculate the mini-batch loss\n",
    "            epoch_loss_total += loss.item()\n",
    "            \n",
    "            loss.backward()         # Calculate the loss with respect to the model parameters\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=agent.parameters(), max_norm=max_grad_norm)   # Prevent the gradients from affecting the model parameters too much and reduce the risk of exploding gradients\n",
    "\n",
    "            optimizer.step()      # Update the model's parameters using the learning rate\n",
    "\n",
    "            # LOGGING LOSS OF CURRENT ITERATION\n",
    "            if (batch_idx + 1) % log_iterations == 0:\n",
    "                log_iteration(batch_idx=(batch_idx + 1), total_batches=train_dataloader_length, loss_value=loss.item())\n",
    "\n",
    "        # CALCULATE AND STORE THE AVERAGE EPOCH LOSS\n",
    "        epoch_avg_loss = epoch_loss_total / train_dataloader_length\n",
    "        history[\"train_loss\"].append(epoch_avg_loss)\n",
    "\n",
    "        # LOG THE AVERAGE LOSS OF THE EPOCH\n",
    "        # log_epoch_iteration(epoch=epoch, avg_epoch_loss=epoch_avg_loss)\n",
    "\n",
    "        # EVALUATE THE MODEL\n",
    "        if (epoch + 1) % eval_iterations == 0:\n",
    "            val_loss = evaluate_model(model=agent, dataloader=validation_dataloader, current_epoch=(epoch + 1), max_epochs=epochs, device=device)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            agent.train()   # Set agent to training mode\n",
    "        \n",
    "    return agent.eval(), history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28949013",
   "metadata": {},
   "source": [
    "## Test Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1949392",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"in_dim\": 22,    # Number of Features as input\n",
    "    \"intermediate_dim\": 128,    \n",
    "    \"out_dim\": 1,   \n",
    "    \"num_blocks\": 12,   # Number of reapeating Layer Blocks\n",
    "    \"dropout_rate\": 0.1     # Rate for dropout layer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4b23eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_policy = Agent(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5dd8cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_LOCATION = \"./models/Agent_02.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "14feac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.load(f=SAVE_LOCATION, weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f4545cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "69256643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(\n",
       "  (linear): Linear(in_features=22, out_features=128, bias=True)\n",
       "  (layers): Sequential(\n",
       "    (0): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "163e4b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file detected, reading from '..\\Data'\n",
      "Train, Test, and Validation csv datasets detected in '..\\Data\\DataSplits.' Skipping generation and loading scaler(s)\n",
      "Initializing Datasets\n",
      "Creating DataLoaders with batch_size (64), num_workers (0), pin_memory (False). Training dataset drop_last: (True)\n",
      "Training DataLoader has (89) batches, Test DataLoader has (4) batches, Validation DataLoader has (4) batches\n",
      "\n",
      "TESTING THE TRAINED POLICY:\n",
      "===============================================\n",
      "Entire Dataset Average Loss: 0.0030 \n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Data Preparation Pipeline --- \n",
    "try:\n",
    "    (train_dataset, test_dataset, validation_dataset, train_dataloader, test_dataloader, validation_dataloader, feature_scaler, label_scaler) = data_pipeline(root_data_dir = \"../Data\", data_file_path = \"OIL_DATASET.csv\", batch_size=64)\n",
    "except ValueError as e:\n",
    "    print(f\"Caught an error: {e}\")\n",
    "\n",
    "# --- Testing Trained Model --- \n",
    "print(\"\\nTESTING THE TRAINED POLICY:\")\n",
    "test_loss = evaluate_model(model=trained_policy, dataloader=test_dataloader, current_epoch=None, max_epochs=None, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4254816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(model, dataset, label_scaler, feature_scaler=None, samples=5) -> None:\n",
    "    \"\"\"Function to sample a model on a given dataset and provide a detailed ouput of the predictions, actual values, and loss for each sample.\n",
    "    Args:\n",
    "        model (Agent): The Model.\n",
    "        dataset (Dataset): The dataset to sample from\n",
    "        samples (int): The number of samples to take from the dataset. If 0 or less, defaults to 5. If more than dataset size, adjusts to dataset size.\"\"\"\n",
    "    loss_fn = torch.nn.L1Loss(reduction='sum')  # Declare the los function to be the Mean Absolute Error loss\n",
    "\n",
    "    if samples <= 0:\n",
    "        print(\"Number of samples must be greater than zero. Setting samples to 5\")\n",
    "        samples = 5\n",
    "\n",
    "    if samples > len(dataset):\n",
    "        print(f\"Requested number of samples ({samples}) exceeds dataset size ({len(dataset)}). Adjusting to dataset size.\")\n",
    "        samples = len(dataset)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for index in range(samples):\n",
    "            (features, label) = dataset[index]\n",
    "            # print(features[:5])   # Print the first 5 features for inspection\n",
    "            pred = model(features)  # Get the models prediction baesd on the input features\n",
    "            loss = loss_fn(pred, label.unsqueeze(dim=-1))   # Calculate the loss, adding an extra dimension to the label to match the prediction shape\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Un-normalize prediction and label using the inverse function and get the scalar value from the numpy arrays\n",
    "            pred_unscaled = label_scaler.inverse_transform(pred.cpu().numpy().reshape(-1, 1))[0, 0]\n",
    "\n",
    "            label_unscaled = label_scaler.inverse_transform(label.cpu().numpy().reshape(-1, 1))[0, 0]\n",
    "\n",
    "            # Un-normalize features if scaler is provided\n",
    "            if feature_scaler is not None:\n",
    "                features_unscaled = feature_scaler.inverse_transform(features.cpu().numpy().reshape(1, -1))[0]\n",
    "                print(f\"Un-normalized features: {features_unscaled}\")\n",
    "            else:\n",
    "                print(f\"Normalized features: {features.numpy()}\")\n",
    "\n",
    "            print(f\"Model predicts: {pred.item():.7f} | Actual: {label:.7f} | Loss: {loss.item():.7f}\")\n",
    "            print(f\"Un-normalized prediction: {pred_unscaled:.4f} | Un-normalized actual: {label_unscaled:.4f}\\n\")\n",
    "    avg_loss = total_loss / samples\n",
    "    print(f\"Average loss over the samples: {avg_loss:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "99ac43e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un-normalized features: [ 122.6       124.07      122.579994  112.02      101.1       102.03999\n",
      " 2521.        444.99997    18.       1009.       3788.       4774.\n",
      " 8562.         73.         73.       2448.        444.99997  1009.\n",
      "   18.       3788.0002   4701.       8489.      ]\n",
      "Model predicts: 0.8267158 | Actual: 0.8185274 | Loss: 0.0081884\n",
      "Un-normalized prediction: 123.8554 | Un-normalized actual: 122.8000\n",
      "\n",
      "Un-normalized features: [  60.81      61.88      59.4       58.53      53.11      53.63\n",
      " 3792.9998   323.        65.       708.      1501.0001  5443.\n",
      " 6944.0005   501.      2755.      3292.       303.00003  708.\n",
      " -201.      1452.      2737.      4189.     ]\n",
      "Model predicts: 0.3338750 | Actual: 0.3451005 | Loss: 0.0112254\n",
      "Un-normalized prediction: 60.3332 | Un-normalized actual: 61.7800\n",
      "\n",
      "Average loss over the samples: 0.0097069\n"
     ]
    }
   ],
   "source": [
    "sample_model(trained_policy, test_dataset, label_scaler, feature_scaler, samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49883946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, features):\n",
    "    \"\"\"Get a prediction from the model for given features.\n",
    "    Args:\n",
    "        model (Agent): The Model.\n",
    "        features (torch.tensor): The input features for prediction.\n",
    "    Returns:\n",
    "        prediction (float): The predicted value.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if not isinstance(features, torch.Tensor):\n",
    "            features = torch.tensor(features, dtype=torch.float)\n",
    "        prediction = model(features.unsqueeze(0))  # Add batch dimension\n",
    "    return prediction.item()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fb01e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.tensor([[0.703135888501742,0.7017001545595055,0.7101257959310453,0.8316066884769173,0.7677131583462,0.7775022143489813,0.43426550948248155,0.5921052631578947,0.014423076923076924,0.3508036338225017,0.5976363636363636,0.2643835616438355,0.5127923976608186,0.11075949367088608,0.014813224559896953,0.46559139784946235,0.5921052631578947,0.3508036338225017,0.4444444444444444,0.5994570135746606,0.7065111758989311,0.699368840659949],[0.46627951993805655,0.4687017001545596,0.474141947507377,0.49382042893493266,0.42108423400240175,0.4279893711248892,0.7965284474445516,0.3665413533834586,0.1394230769230769,0.15094339622641506,0.527090909090909,0.5118721461187214,0.5411184210526316,0.6787974683544303,0.41155002146844144,0.7408602150537634,0.30263157894736836,0.15094339622641506,0.3170731707317073,0.5236199095022624,0.4586977648202138,0.5116819842763813]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c64de248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7077],\n",
       "        [0.4703]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.get_prediction(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14bab2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pred: 0.7076724171638489\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model pred: {get_prediction(trained_policy, sample_input)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
